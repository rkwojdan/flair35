from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functiontry:    from pathlib import Pathexcept ImportError:    from pathlib2 import Pathfrom typing import List, Unionimport datetimeimport randomimport loggingfrom torch.optim.sgd import SGDimport flairimport flair.nnfrom flair.data import Sentence, Token, MultiCorpus, Corpusfrom flair.models import TextClassifier, SequenceTaggerfrom flair.training_utils import Metric, init_output_file, WeightExtractor, clear_embeddings, EvaluationMetric, log_line, add_file_handlerfrom flair.optim import *log = logging.getLogger('flair')class ModelTrainer(object):    def __init__(self, model, corpus, optimizer=SGD, epoch=0, loss=10000.0, optimizer_state=None, scheduler_state=None):        self.model = model        self.corpus = corpus        self.optimizer = optimizer        self.epoch = epoch        self.loss = loss        self.scheduler_state = scheduler_state        self.optimizer_state = optimizer_state    def train(self, base_path, evaluation_metric=EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, eval_mini_batch_size=None, max_epochs=100, anneal_factor=0.5, patience=3, anneal_against_train_loss=True, train_with_dev=False, monitor_train=False, embeddings_in_memory=True, checkpoint=False, save_final_model=True, anneal_with_restarts=False, test_mode=False, param_selection_mode=False, **kwargs):        if (eval_mini_batch_size is None):            eval_mini_batch_size = mini_batch_size        if (type(base_path) is str):            base_path = Path(base_path)        add_file_handler(log, (base_path / 'training.log'))        log_line(log)        log.info(            ''.join(['Evaluation method: ', '{}'.format(evaluation_metric.name)]))        if (not param_selection_mode):            loss_txt = init_output_file(base_path, 'loss.tsv')            with open(loss_txt, 'a') as f:                f.write(''.join(['EPOCH\tTIMESTAMP\tBAD_EPOCHS\tLEARNING_RATE\tTRAIN_LOSS\t', '{}'.format(Metric.tsv_header(                    'TRAIN')), '\tDEV_LOSS\t', '{}'.format(Metric.tsv_header('DEV')), '\tTEST_LOSS\t', '{}'.format(Metric.tsv_header('TEST')), '\n']))            weight_extractor = WeightExtractor(base_path)        optimizer = self.optimizer(            self.model.parameters(), lr=learning_rate, **kwargs)        if (self.optimizer_state is not None):            optimizer.load_state_dict(self.optimizer_state)        anneal_mode = ('min' if anneal_against_train_loss else 'max')        if isinstance(optimizer, (AdamW, SGDW)):            scheduler = ReduceLRWDOnPlateau(                optimizer, factor=anneal_factor, patience=patience, mode=anneal_mode, verbose=True)        else:            scheduler = ReduceLROnPlateau(                optimizer, factor=anneal_factor, patience=patience, mode=anneal_mode, verbose=True)        if (self.scheduler_state is not None):            scheduler.load_state_dict(self.scheduler_state)        train_data = self.corpus.train        if train_with_dev:            train_data.extend(self.corpus.dev)        dev_score_history = []        dev_loss_history = []        train_loss_history = []        try:            previous_learning_rate = learning_rate            for epoch in range((0 + self.epoch), (max_epochs + self.epoch)):                log_line(log)                try:                    bad_epochs = scheduler.num_bad_epochs                except:                    bad_epochs = 0                for group in optimizer.param_groups:                    learning_rate = group['lr']                if ((learning_rate != previous_learning_rate) and anneal_with_restarts and (base_path / 'best-model.pt').exists()):                    log.info('resetting to best model')                    self.model.load_from_file((base_path / 'best-model.pt'))                previous_learning_rate = learning_rate                if (learning_rate < 0.0001):                    log_line(log)                    log.info('learning rate too small - quitting training!')                    log_line(log)                    break                if (not test_mode):                    random.shuffle(train_data)                batches = [train_data[x:(x + mini_batch_size)]                           for x in range(0, len(train_data), mini_batch_size)]                self.model.train()                train_loss = 0                seen_sentences = 0                modulo = max(1, int((len(batches) / 10)))                for (batch_no, batch) in enumerate(batches):                    loss = self.model.forward_loss(batch)                    optimizer.zero_grad()                    loss.backward()                    torch.nn.utils.clip_grad_norm_(                        self.model.parameters(), 5.0)                    optimizer.step()                    seen_sentences += len(batch)                    train_loss += loss.item()                    clear_embeddings(batch, also_clear_word_embeddings=(                        not embeddings_in_memory))                    if ((batch_no % modulo) == 0):                        log.info(''.join(['epoch ', '{}'.format((epoch + 1)), ' - iter ', '{}'.format(batch_no), '/',                                          '{}'.format(len(batches)), ' - loss ', '{:.8f}'.format((train_loss / seen_sentences))]))                        iteration = ((epoch * len(batches)) + batch_no)                        if (not param_selection_mode):                            weight_extractor.extract_weights(                                self.model.state_dict(), iteration)                train_loss /= len(train_data)                self.model.eval()                log_line(log)                log.info(''.join(['EPOCH ', '{}'.format((epoch + 1)), ' done: loss ', '{:.4f}'.format(                    train_loss), ' - lr ', '{:.4f}'.format(learning_rate), ' - bad epochs ', '{}'.format(bad_epochs)]))                dev_metric = None                dev_loss = '_'                train_metric = None                test_metric = None                if monitor_train:                    (train_metric, train_loss) = self._calculate_evaluation_results_for(                        'TRAIN', self.corpus.train, evaluation_metric, embeddings_in_memory, eval_mini_batch_size)                if (not train_with_dev):                    (dev_metric, dev_loss) = self._calculate_evaluation_results_for(                        'DEV', self.corpus.dev, evaluation_metric, embeddings_in_memory, eval_mini_batch_size)                if ((not param_selection_mode) and self.corpus.test):                    (test_metric, test_loss) = self._calculate_evaluation_results_for('TEST', self.corpus.test,                                                                                      evaluation_metric, embeddings_in_memory, eval_mini_batch_size, (base_path / 'test.tsv'))                if (not param_selection_mode):                    with open(loss_txt, 'a') as f:                        train_metric_str = (train_metric.to_tsv() if (                            train_metric is not None) else Metric.to_empty_tsv())                        dev_metric_str = (dev_metric.to_tsv() if (                            dev_metric is not None) else Metric.to_empty_tsv())                        test_metric_str = (test_metric.to_tsv() if (                            test_metric is not None) else Metric.to_empty_tsv())                        f.write(''.join(['{}'.format(epoch), '\t', '{:%H:%M:%S}'.format(datetime.datetime.now()), '\t', '{}'.format(bad_epochs), '\t', '{:.4f}'.format(learning_rate), '\t', '{}'.format(                            train_loss), '\t', '{}'.format(train_metric_str), '\t', '{}'.format(dev_loss), '\t', '{}'.format(dev_metric_str), '\t_\t', '{}'.format(test_metric_str), '\n']))                dev_score = 0.0                if (not train_with_dev):                    if (evaluation_metric == EvaluationMetric.MACRO_ACCURACY):                        dev_score = dev_metric.macro_avg_accuracy()                    elif (evaluation_metric == EvaluationMetric.MICRO_ACCURACY):                        dev_score = dev_metric.micro_avg_accuracy()                    elif (evaluation_metric == EvaluationMetric.MACRO_F1_SCORE):                        dev_score = dev_metric.macro_avg_f_score()                    else:                        dev_score = dev_metric.micro_avg_f_score()                    dev_score_history.append(dev_score)                    dev_loss_history.append(dev_loss.item())                current_score = (                    train_loss if anneal_against_train_loss else dev_score)                scheduler.step(current_score)                train_loss_history.append(train_loss)                if (checkpoint and (not param_selection_mode)):                    self.model.save_checkpoint((base_path / 'checkpoint.pt'), optimizer.state_dict(                    ), scheduler.state_dict(), (epoch + 1), train_loss)                if ((not train_with_dev) and (not param_selection_mode) and (current_score == scheduler.best)):                    self.model.save((base_path / 'best-model.pt'))            if (save_final_model and (not param_selection_mode)):                self.model.save((base_path / 'final-model.pt'))        except KeyboardInterrupt:            log_line(log)            log.info('Exiting from training early.')            if (not param_selection_mode):                log.info('Saving model ...')                self.model.save((base_path / 'final-model.pt'))                log.info('Done.')        if self.corpus.test:            final_score = self.final_test(                base_path, embeddings_in_memory, evaluation_metric, eval_mini_batch_size)        else:            final_score = 0            log.info('Test data not provided setting final score to 0')        return {'test_score': final_score, 'dev_score_history': dev_score_history, 'train_loss_history': train_loss_history, 'dev_loss_history': dev_loss_history}    def final_test(self, base_path, embeddings_in_memory, evaluation_metric, eval_mini_batch_size):        log_line(log)        log.info('Testing using best model ...')        self.model.eval()        if (base_path / 'best-model.pt').exists():            if isinstance(self.model, TextClassifier):                self.model = TextClassifier.load_from_file(                    (base_path / 'best-model.pt'))            if isinstance(self.model, SequenceTagger):                self.model = SequenceTagger.load_from_file(                    (base_path / 'best-model.pt'))        (test_metric, test_loss) = self.evaluate(self.model, self.corpus.test,                                                 eval_mini_batch_size=eval_mini_batch_size, embeddings_in_memory=embeddings_in_memory)        log.info(''.join(['MICRO_AVG: acc ', '{}'.format(test_metric.micro_avg_accuracy(        )), ' - f1-score ', '{}'.format(test_metric.micro_avg_f_score())]))        log.info(''.join(['MACRO_AVG: acc ', '{}'.format(test_metric.macro_avg_accuracy(        )), ' - f1-score ', '{}'.format(test_metric.macro_avg_f_score())]))        for class_name in test_metric.get_classes():            log.info(''.join(['{:<10}'.format(class_name), ' tp: ', '{}'.format(test_metric.get_tp(class_name)), ' - fp: ', '{}'.format(test_metric.get_fp(class_name)), ' - fn: ', '{}'.format(test_metric.get_fn(class_name)), ' - tn: ', '{}'.format(test_metric.get_tn(class_name)),                              ' - precision: ', '{:.4f}'.format(test_metric.precision(class_name)), ' - recall: ', '{:.4f}'.format(test_metric.recall(class_name)), ' - accuracy: ', '{:.4f}'.format(test_metric.accuracy(class_name)), ' - f1-score: ', '{:.4f}'.format(test_metric.f_score(class_name))]))        log_line(log)        if (type(self.corpus) is MultiCorpus):            for subcorpus in self.corpus.corpora:                log_line(log)                self._calculate_evaluation_results_for(                    subcorpus.name, subcorpus.test, evaluation_metric, embeddings_in_memory, eval_mini_batch_size, (base_path / 'test.tsv'))        if (evaluation_metric == EvaluationMetric.MACRO_ACCURACY):            final_score = test_metric.macro_avg_accuracy()        elif (evaluation_metric == EvaluationMetric.MICRO_ACCURACY):            final_score = test_metric.micro_avg_accuracy()        elif (evaluation_metric == EvaluationMetric.MACRO_F1_SCORE):            final_score = test_metric.macro_avg_f_score()        else:            final_score = test_metric.micro_avg_f_score()        return final_score    def _calculate_evaluation_results_for(self, dataset_name, dataset, evaluation_metric, embeddings_in_memory, eval_mini_batch_size, out_path=None):        (metric, loss) = ModelTrainer.evaluate(self.model, dataset, eval_mini_batch_size=eval_mini_batch_size,                                               embeddings_in_memory=embeddings_in_memory, out_path=out_path)        if ((evaluation_metric == EvaluationMetric.MACRO_ACCURACY) or (evaluation_metric == EvaluationMetric.MACRO_F1_SCORE)):            f_score = metric.macro_avg_f_score()            acc = metric.macro_avg_accuracy()        else:            f_score = metric.micro_avg_f_score()            acc = metric.micro_avg_accuracy()        log.info(''.join(['{:<5}'.format(dataset_name), ': loss ', '{:.8f}'.format(            loss), ' - f-score ', '{:.4f}'.format(f_score), ' - acc ', '{:.4f}'.format(acc)]))        return (metric, loss)    @staticmethod    def evaluate(model, data_set, eval_mini_batch_size=32, embeddings_in_memory=True, out_path=None):        if isinstance(model, TextClassifier):            return ModelTrainer._evaluate_text_classifier(model, data_set, eval_mini_batch_size, embeddings_in_memory, out_path)        elif isinstance(model, SequenceTagger):            return ModelTrainer._evaluate_sequence_tagger(model, data_set, eval_mini_batch_size, embeddings_in_memory, out_path)    @staticmethod    def _evaluate_sequence_tagger(model, sentences, eval_mini_batch_size=32, embeddings_in_memory=True, out_path=None):        with torch.no_grad():            eval_loss = 0            batch_no = 0            batches = [sentences[x:(x + eval_mini_batch_size)]                       for x in range(0, len(sentences), eval_mini_batch_size)]            metric = Metric('Evaluation')            lines = []            for batch in batches:                batch_no += 1                (tags, loss) = model.forward_labels_and_loss(batch)                eval_loss += loss                for (sentence, sent_tags) in zip(batch, tags):                    for (token, tag) in zip(sentence.tokens, sent_tags):                        token = token                        token.add_tag_label('predicted', tag)                        eval_line = '{} {} {} {}\n'.format(token.text, token.get_tag(                            model.tag_type).value, tag.value, tag.score)                        lines.append(eval_line)                    lines.append('\n')                for sentence in batch:                    gold_tags = [(tag.tag, str(tag))                                 for tag in sentence.get_spans(model.tag_type)]                    predicted_tags = [(tag.tag, str(tag))                                      for tag in sentence.get_spans('predicted')]                    for (tag, prediction) in predicted_tags:                        if ((tag, prediction) in gold_tags):                            metric.add_tp(tag)                        else:                            metric.add_fp(tag)                    for (tag, gold) in gold_tags:                        if ((tag, gold) not in predicted_tags):                            metric.add_fn(tag)                        else:                            metric.add_tn(tag)                clear_embeddings(batch, also_clear_word_embeddings=(                    not embeddings_in_memory))            eval_loss /= len(sentences)            if (out_path is not None):                with open(out_path, 'w', encoding='utf-8') as outfile:                    outfile.write(''.join(lines))            return (metric, eval_loss)    @staticmethod    def _evaluate_text_classifier(model, sentences, eval_mini_batch_size=32, embeddings_in_memory=False, out_path=None):        with torch.no_grad():            eval_loss = 0            batches = [sentences[x:(x + eval_mini_batch_size)]                       for x in range(0, len(sentences), eval_mini_batch_size)]            metric = Metric('Evaluation')            lines = []            for batch in batches:                (labels, loss) = model.forward_labels_and_loss(batch)                clear_embeddings(batch, also_clear_word_embeddings=(                    not embeddings_in_memory))                eval_loss += loss                sentences_for_batch = [sent.to_plain_string()                                       for sent in batch]                confidences_for_batch = [                    [label.score for label in sent_labels] for sent_labels in labels]                predictions_for_batch = [                    [label.value for label in sent_labels] for sent_labels in labels]                true_values_for_batch = [                    sentence.get_label_names() for sentence in batch]                available_labels = model.label_dictionary.get_items()                for (sentence, confidence, prediction, true_value) in zip(sentences_for_batch, confidences_for_batch, predictions_for_batch, true_values_for_batch):                    eval_line = '{}\t{}\t{}\t{}\n'.format(                        sentence, true_value, prediction, confidence)                    lines.append(eval_line)                for (predictions_for_sentence, true_values_for_sentence) in zip(predictions_for_batch, true_values_for_batch):                    ModelTrainer._evaluate_sentence_for_text_classification(                        metric, available_labels, predictions_for_sentence, true_values_for_sentence)            eval_loss /= len(sentences)            if (out_path is not None):                with open(out_path, 'w', encoding='utf-8') as outfile:                    outfile.write(''.join(lines))            return (metric, eval_loss)    @staticmethod    def _evaluate_sentence_for_text_classification(metric, available_labels, predictions, true_values):        for label in available_labels:            if ((label in predictions) and (label in true_values)):                metric.add_tp(label)            elif ((label in predictions) and (label not in true_values)):                metric.add_fp(label)            elif ((label not in predictions) and (label in true_values)):                metric.add_fn(label)            elif ((label not in predictions) and (label not in true_values)):                metric.add_tn(label)    @staticmethod    def load_from_checkpoint(checkpoint_file, model_type, corpus, optimizer=SGD):        if (model_type == 'SequenceTagger'):            checkpoint = SequenceTagger.load_checkpoint(checkpoint_file)            return ModelTrainer(checkpoint['model'], corpus, optimizer, epoch=checkpoint['epoch'], loss=checkpoint['loss'], optimizer_state=checkpoint['optimizer_state_dict'], scheduler_state=checkpoint['scheduler_state_dict'])        if (model_type == 'TextClassifier'):            checkpoint = TextClassifier.load_checkpoint(checkpoint_file)            return ModelTrainer(checkpoint['model'], corpus, optimizer, epoch=checkpoint['epoch'], loss=checkpoint['loss'], optimizer_state=checkpoint['optimizer_state_dict'], scheduler_state=checkpoint['scheduler_state_dict'])        raise ValueError(            'Incorrect model type! Use one of the following: "SequenceTagger", "TextClassifier".')    def find_learning_rate(self, base_path, file_name='learning_rate.tsv', start_learning_rate=1e-07, end_learning_rate=10, iterations=100, mini_batch_size=32, stop_early=True, smoothing_factor=0.98, **kwargs):        best_loss = None        moving_avg_loss = 0        if (type(base_path) is str):            base_path = Path(base_path)        learning_rate_tsv = init_output_file(base_path, file_name)        with open(learning_rate_tsv, 'a') as f:            f.write('ITERATION\tTIMESTAMP\tLEARNING_RATE\tTRAIN_LOSS\n')        optimizer = self.optimizer(            self.model.parameters(), lr=start_learning_rate, **kwargs)        train_data = self.corpus.train        random.shuffle(train_data)        batches = [train_data[x:(x + mini_batch_size)]                   for x in range(0, len(train_data), mini_batch_size)][:iterations]        scheduler = ExpAnnealLR(optimizer, end_learning_rate, iterations)        model_state = self.model.state_dict()        model_device = next(self.model.parameters()).device        self.model.train()        for (itr, batch) in enumerate(batches):            loss = self.model.forward_loss(batch)            optimizer.zero_grad()            loss.backward()            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)            optimizer.step()            scheduler.step()            learning_rate = scheduler.get_lr()[0]            loss_item = loss.item()            if (itr == 0):                best_loss = loss_item            else:                if (smoothing_factor > 0):                    moving_avg_loss = (                        (smoothing_factor * moving_avg_loss) + ((1 - smoothing_factor) * loss_item))                    loss_item = (moving_avg_loss /                                 (1 - (smoothing_factor ** (itr + 1))))                if (loss_item < best_loss):                    best_loss = loss            if (stop_early and ((loss_item > (4 * best_loss)) or torch.isnan(loss))):                log_line(log)                log.info('loss diverged - stopping early!')                break            with open(learning_rate_tsv, 'a') as f:                f.write(''.join(['{}'.format(itr), '\t', '{:%H:%M:%S}'.format(datetime.datetime.now(                )), '\t', '{}'.format(learning_rate), '\t', '{}'.format(loss_item), '\n']))        self.model.load_state_dict(model_state)        self.model.to(model_device)        log_line(log)        log.info(''.join(            ['learning rate finder finished - plot ', '{}'.format(learning_rate_tsv)]))        log_line(log)        return Path(learning_rate_tsv)